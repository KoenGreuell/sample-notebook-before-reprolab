{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c89227-e15b-4e8b-9e3b-f5d020b07211",
   "metadata": {},
   "source": [
    "# ReproLab Demo\n",
    "\n",
    "Welcome to ReproLab! This extension helps you make your research more reproducible.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Create Experiments**: Automatically save immutable snapshots of your code under `git` tags to preserve the **exact code and outputs**\n",
    "- **Manage Dependencies**: Automatically gather and pin **exact package versions**, so that others can set up your environment with one command\n",
    "- **Cache Data**: Call external API/load manually dataset only once, caching function will handle the rest\n",
    "- **Archive Data**: Caching function can also preserve the compressed data in *AWS S3*, so you always know what data was used and reduce the API calls\n",
    "- **Publishing guide**: The reproducibility checklist & automated generation of reproducability package make publishing to platforms such as Zenodo very easy\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Use the sidebar to view ReproLab features\n",
    "2. Create virtual environment and pin your dependencies, go to reprolab section `Create reproducible environment` \n",
    "3. Create an experiment to save your current state, go to reprolab section `Create experiment`\n",
    "4. Archive your data for long-term storage, go to reprolab section `Demo` and play around with it.\n",
    "5. Publish your work when ready, remember to use reproducability checklist from the section `Reproducibility Checklist`\n",
    "\n",
    "## Example Usage of persistio decorator\n",
    "\n",
    "To cache and archive the datasets you use, both from local files and APIs we developed a simple decorator that put over your function that gets the datasets caches the file both locally and in the cloud so that the dataset you use is archived and the number of calls to external APIs is minimal and you don't need to keep the file around after you run it once.\n",
    "\n",
    "Here is an example using one of NASA open APIs. If you want to test it out yourself, you can copy the code, but you need to provide bucket name and access and secret key in the left-hand panel using the `AWS S3 Configuration` section.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# The two lines below is all that you need to add\n",
    "from reprolab.experiment import persistio\n",
    "@persistio()\n",
    "def get_exoplanets_data_from_nasa():\n",
    "    url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT TOP 10\n",
    "        pl_name AS planet_name,\n",
    "        hostname AS host_star,\n",
    "        pl_orbper AS orbital_period_days,\n",
    "        pl_rade AS planet_radius_earth,\n",
    "        disc_year AS discovery_year\n",
    "    FROM\n",
    "        ps\n",
    "    WHERE\n",
    "        default_flag = 1\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        df = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        print(df)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    return df\n",
    "\n",
    "exoplanets_data = get_exoplanets_data_from_nasa()\n",
    "```\n",
    "\n",
    "If you run this cell twice you will notice from the logs that the second time file was read from the compressed file in the cache. If you were to lose access to local cache (e.g. by pulling the repository using different device) `persistio` would fetch the data from the cloud archive.\n",
    "\n",
    "\n",
    "For more information, visit our [documentation](https://github.com/your-repo/reprolab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "595c7797-f6e9-4589-800e-ac1585975c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>planet_name</th>\n",
       "      <th>host_star</th>\n",
       "      <th>orbital_period_days</th>\n",
       "      <th>planet_radius_earth</th>\n",
       "      <th>discovery_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kepler-6 b</td>\n",
       "      <td>Kepler-6</td>\n",
       "      <td>3.234700</td>\n",
       "      <td>14.616536</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kepler-491 b</td>\n",
       "      <td>Kepler-491</td>\n",
       "      <td>4.225385</td>\n",
       "      <td>8.920000</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kepler-29 c</td>\n",
       "      <td>Kepler-29</td>\n",
       "      <td>13.286130</td>\n",
       "      <td>2.340000</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kepler-257 b</td>\n",
       "      <td>Kepler-257</td>\n",
       "      <td>2.382667</td>\n",
       "      <td>2.610000</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kepler-216 b</td>\n",
       "      <td>Kepler-216</td>\n",
       "      <td>7.693641</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kepler-32 c</td>\n",
       "      <td>Kepler-32</td>\n",
       "      <td>8.752200</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kepler-259 c</td>\n",
       "      <td>Kepler-259</td>\n",
       "      <td>36.924931</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kepler-148 c</td>\n",
       "      <td>Kepler-148</td>\n",
       "      <td>4.180043</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kepler-222 d</td>\n",
       "      <td>Kepler-222</td>\n",
       "      <td>28.081912</td>\n",
       "      <td>3.690000</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kepler-179 b</td>\n",
       "      <td>Kepler-179</td>\n",
       "      <td>2.735926</td>\n",
       "      <td>1.640000</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    planet_name   host_star  orbital_period_days  planet_radius_earth  \\\n",
       "0    Kepler-6 b    Kepler-6             3.234700            14.616536   \n",
       "1  Kepler-491 b  Kepler-491             4.225385             8.920000   \n",
       "2   Kepler-29 c   Kepler-29            13.286130             2.340000   \n",
       "3  Kepler-257 b  Kepler-257             2.382667             2.610000   \n",
       "4  Kepler-216 b  Kepler-216             7.693641             2.350000   \n",
       "5   Kepler-32 c   Kepler-32             8.752200             2.000000   \n",
       "6  Kepler-259 c  Kepler-259            36.924931             2.700000   \n",
       "7  Kepler-148 c  Kepler-148             4.180043             3.600000   \n",
       "8  Kepler-222 d  Kepler-222            28.081912             3.690000   \n",
       "9  Kepler-179 b  Kepler-179             2.735926             1.640000   \n",
       "\n",
       "   discovery_year  \n",
       "0            2009  \n",
       "1            2016  \n",
       "2            2011  \n",
       "3            2014  \n",
       "4            2014  \n",
       "5            2011  \n",
       "6            2014  \n",
       "7            2014  \n",
       "8            2014  \n",
       "9            2014  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# The two lines below is all that you need to add\n",
    "from reprolab.experiment import persistio\n",
    "@persistio()\n",
    "def get_exoplanets_data_from_nasa():\n",
    "    url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT TOP 10\n",
    "        pl_name AS planet_name,\n",
    "        hostname AS host_star,\n",
    "        pl_orbper AS orbital_period_days,\n",
    "        pl_rade AS planet_radius_earth,\n",
    "        disc_year AS discovery_year\n",
    "    FROM\n",
    "        ps\n",
    "    WHERE\n",
    "        default_flag = 1\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        df = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        print(df)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    return df\n",
    "\n",
    "exoplanets_data = get_exoplanets_data_from_nasa()\n",
    "exoplanets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fdcf654-7fd8-444d-a1cf-34a0e1851c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(1*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "277497b3-f077-471b-941d-e03345ad6d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/koen-greuell/jupyter-reprolab/myenv/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/koen-greuell/jupyter-reprolab/myenv/lib/python3.13/site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/koen-greuell/jupyter-reprolab/myenv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/koen-greuell/jupyter-reprolab/myenv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/koen-greuell/jupyter-reprolab/myenv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/koen-greuell/jupyter-reprolab/myenv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f1b2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CONTAINERISE\n",
    "# parametrization - v95\n",
    "# Run specific - fill in text to preset, keep empty to prompt\n",
    "import os\n",
    "\n",
    "\n",
    "param_radar = \"HRW\"  # DHL | HRW | DBL\n",
    "param_start_date = (\n",
    "    \"2019-12-31T23:00+00:00\"  # %Y%m%dT%H:%M+TZ; 2019-12-31T23:00+00:00\n",
    ")\n",
    "param_end_date = (\n",
    "    \"2020-01-01T01:00+00:00\"  # %Y%m%dT%H:%M+TZ; 2020-01-01T01:00+00:00\n",
    ")\n",
    "param_concurrency = 5\n",
    "param_interval_in_minutes = 60\n",
    "# This is to control uploads / cleaning\n",
    "# Move results to S3?\n",
    "param_upload_results = \"True\"\n",
    "# Store and retrieve data from the public S3 / MinIO bucket\n",
    "param_public_minio_data = 0\n",
    "# Remove input after processing KNMI format to ODIM format\n",
    "param_clean_knmi_input = \"True\"\n",
    "# Should we remove the final Polar Volumes after producing a VP and or RBC\n",
    "param_clean_pvol_output = \"True\"\n",
    "#\n",
    "param_clean_vp_output = \"True\"\n",
    "# The maximum number of timepoints to download and create vertical profiles and polar volumes from\n",
    "param_maximum_KNMI_files = 4\n",
    "\n",
    "# Param\n",
    "### User specific, not neccesarily run specific.\n",
    "#### Update: Perhaps some of these userinfo should be hardcoded parameters. I mean\n",
    "#### Not something we'd enter every single time but should be set at setup time.\n",
    "param_user_number = \"001\"\n",
    "\n",
    "#### Visualization parameters\n",
    "param_dtype = \"pvol\"  # pvol | vp\n",
    "param_country = \"NL\"  # only NL\n",
    "param_year = \"2023\"  # as string, YYYY\n",
    "param_month = \"12\"  # as string, mm\n",
    "param_day = \"31\"  # as string, dd\n",
    "\n",
    "# Perhaps param prefix works better\n",
    "param_prefix = \"NL/DHL/2023/12/31\"\n",
    "\n",
    "# parameters\n",
    "param_elevation = 3  #\n",
    "param_param = \"VRADH\"  # I know...VRADH, DBZH, TH, WRADH, RHOHV, DBZ...\n",
    "param_imtype = \"ppi\"  # Likely only type so far. PVOL -> PPI, VP -> VPTS. Future there would be more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "144465b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SecretsProvider'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# DO NOT CONTAINERISE\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Get secrets, if they dont exist, set them\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSecretsProvider\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SecretsProvider\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgetpass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[32m      7\u001b[39m secrets_provider = SecretsProvider(input_func=getpass)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'SecretsProvider'"
     ]
    }
   ],
   "source": [
    "# DO NOT CONTAINERISE\n",
    "# Get secrets, if they dont exist, set them\n",
    "from SecretsProvider import SecretsProvider\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "secrets_provider = SecretsProvider(input_func=getpass)\n",
    "secret_key_knmi_api = secrets_provider.get_secret(\"secret_knmi_api_key\")\n",
    "secret_minio_access_key = secrets_provider.get_secret(\n",
    "    \"secret_minio_access_key\"\n",
    ")\n",
    "secret_minio_secret_key = secrets_provider.get_secret(\n",
    "    \"secret_minio_secret_key\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5fe17-d03d-4623-a368-4ab12d17ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CONTAINERIZE\n",
    "# Use to re-set existing keys\n",
    "from SecretsProvider import SecretsProvider\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "secrets_provider = SecretsProvider(input_func=getpass)\n",
    "secrets_provider.set_secret(\"secret_knmi_api_key\")\n",
    "secrets_provider.set_secret(\"secret_minio_access_key\")\n",
    "secrets_provider.set_secret(\"secret_minio_secret_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CONTAINERISE\n",
    "# configuration - v95\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "conf_minio_user_bucket_name = \"naa-vre-user-data\"  # the user bucket name\n",
    "conf_minio_tutorial_prefix = \"ravl-tutorial\"\n",
    "conf_minio_public_bucket_name = \"naa-vre-public\"  # the public bucket name\n",
    "conf_minio_public_root_prefix = \"vl-vol2bird\"\n",
    "conf_minio_public_conf_prefix = \"vl-vol2bird/conf\"\n",
    "conf_minio_public_conf_radar_db_object_name = (\n",
    "    \"vl-vol2bird/conf/OPERA_RADARS_DB.json\"\n",
    ")\n",
    "conf_minio_endpoint = \"scruffy.lab.uvalight.net:9000\"\n",
    "\n",
    "### Directories\n",
    "conf_local_root = \"/tmp/data\"\n",
    "conf_local_knmi = \"/tmp/data/knmi\"\n",
    "conf_local_odim = \"/tmp/data/odim\"\n",
    "conf_local_vp = \"/tmp/data/vp\"\n",
    "conf_local_ppi = \"/tmp/data/ppi\"\n",
    "conf_local_vpts = \"/tmp/data/vpts\"\n",
    "conf_local_conf = \"/tmp/data/conf\"\n",
    "conf_local_radar_db = \"/tmp/data/conf/OPERA_RADARS_DB.json\"\n",
    "conf_local_visualization_input = \"/tmp/data/visualizations/input\"\n",
    "conf_local_visualization_output = \"/tmp/data/visualizatons/output\"\n",
    "\n",
    "conf_pvol_output_prefix = \"pvol\"\n",
    "conf_vp_output_prefix = \"vp\"\n",
    "conf_ppi_output_prefix = \"ppi\"\n",
    "conf_vpts_output_prefix = \"vpts\"\n",
    "conf_user_directory = \"user\"\n",
    "\n",
    "# radar configuration for the KNMI api\n",
    "# Rewritten in a long format without page breaks. This is to prevent\n",
    "# the code analyzer to yield an error.\n",
    "# datasetName, datasetVersion, api_url, radar code (odim)\n",
    "conf_herwijnen = [\n",
    "    \"radar_volume_full_herwijnen\",\n",
    "    1.0,\n",
    "    \"https://api.dataplatform.knmi.nl/open-data/v1/datasets/radar_volume_full_herwijnen/versions/1.0/files\",\n",
    "    \"NL/HRW\",\n",
    "]\n",
    "conf_denhelder = [\n",
    "    \"radar_volume_full_denhelder\",\n",
    "    2.0,\n",
    "    \"https://api.dataplatform.knmi.nl/open-data/v1/datasets/radar_volume_denhelder/versions/2.0/files\",\n",
    "    \"NL/DHL\",\n",
    "]\n",
    "conf_radars = {\n",
    "    \"hrw\": conf_herwijnen,\n",
    "    \"herwijnen\": conf_herwijnen,\n",
    "    \"dhl\": conf_denhelder,\n",
    "    \"den helder\": conf_denhelder,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef057f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CONTAINERISE - v60\n",
    "# Set initial resource in minio, the conf in common\n",
    "debug = False\n",
    "from minio import Minio, S3Error\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "minioClient = Minio(\n",
    "    endpoint=conf_minio_endpoint,\n",
    "    access_key=secret_minio_access_key,\n",
    "    secret_key=secret_minio_secret_key,\n",
    "    secure=True,\n",
    ")\n",
    "# Stat object to see if it is there. We could also just try a try except on fget\n",
    "# However, canonically this seems better to stat\n",
    "# Cast to str as stat object doesnt like posix\n",
    "try:\n",
    "    radar_db_stat = minioClient.stat_object(\n",
    "        bucket_name=pathlib.Path(conf_minio_public_bucket_name).as_posix(),\n",
    "        object_name=pathlib.Path(\n",
    "            conf_minio_public_conf_radar_db_object_name\n",
    "        ).as_posix(),\n",
    "    )\n",
    "    print(\n",
    "        f\"Reference file found [{pathlib.Path(conf_minio_public_bucket_name).as_posix()}]/{pathlib.Path(conf_minio_public_conf_radar_db_object_name).as_posix()}\"\n",
    "    )\n",
    "    print(radar_db_stat)\n",
    "\n",
    "except S3Error as e:\n",
    "    print(\n",
    "        f\"Failed to find reference file [{pathlib.Path(conf_minio_public_bucket_name).as_posix()}]/{pathlib.Path(conf_minio_public_conf_radar_db_object_name).as_posix()}\"\n",
    "    )\n",
    "    if debug:\n",
    "        print(f\"{e=}\")\n",
    "\n",
    "    file_stat = os.stat(\"/home/jovyan/data/conf/OPERA_RADARS_DB.json\")\n",
    "    with open(\n",
    "        \"/home/jovyan/data/conf/OPERA_RADARS_DB.json\", mode=\"rb\"\n",
    "    ) as file_data:\n",
    "        put_result = minioClient.put_object(\n",
    "            bucket_name=pathlib.Path(conf_minio_public_bucket_name).as_posix(),\n",
    "            object_name=pathlib.Path(\n",
    "                conf_minio_public_conf_radar_db_object_name\n",
    "            ).as_posix(),\n",
    "            data=file_data,\n",
    "            length=file_stat.st_size,\n",
    "        )\n",
    "    print(f\"{put_result=}\")\n",
    "    # check put result if we indeed uploaded succesfully\n",
    "    # if we dont try except we will lazily evaluate the result, as fail likely yields hard crash\n",
    "    print(\"Succesfully uploaded reference file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158be3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializer\n",
    "import pathlib\n",
    "\n",
    "# Make directories on shared (local) storage\n",
    "for local_dir in [\n",
    "    conf_local_root,\n",
    "    conf_local_knmi,\n",
    "    conf_local_odim,\n",
    "    conf_local_vp,\n",
    "    conf_local_conf,\n",
    "]:\n",
    "    local_dir = pathlib.Path(local_dir)\n",
    "    if not local_dir.exists():\n",
    "        local_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Reference files\n",
    "if not pathlib.Path(conf_local_radar_db).exists():\n",
    "    from minio import Minio, S3Error\n",
    "\n",
    "    minioClient = Minio(\n",
    "        endpoint=conf_minio_endpoint,\n",
    "        access_key=secret_minio_access_key,\n",
    "        secret_key=secret_minio_secret_key,\n",
    "        secure=True,\n",
    "    )\n",
    "    print(f\"{conf_local_radar_db} not found, downloading\")\n",
    "    minioClient.fget_object(\n",
    "        bucket_name=conf_minio_public_bucket_name,\n",
    "        object_name=conf_minio_public_conf_radar_db_object_name,\n",
    "        file_path=conf_local_radar_db,\n",
    "    )\n",
    "\n",
    "# Now produce a variable which acts as a marker for the workflow manager\n",
    "# We can then drag a line from the configuration / initializer\n",
    "# and time the start of the rest of the workflow\n",
    "# If you decide to make different sets of configurations, you can store them\n",
    "# and decide per workflow which config to attach\n",
    "init_complete = \"Yes\"  # Cant sent bool\n",
    "print(\"Finished initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list-knmi-files\n",
    "\"\"\"\n",
    "consume dummy var from config to signal workflow start\n",
    "There is something dodgy going on with how\n",
    "strings are being passed around.\n",
    "The string \"Yes\" is being sent as '\"Yes\"'\n",
    "So, to prevent extra quotes being introduced\n",
    "we eval init_complete first before\n",
    "we test if it contains \"Yes\"\n",
    "\"\"\"\n",
    "# Libraries\n",
    "import requests\n",
    "\n",
    "\n",
    "def validate_api_errors():\n",
    "    if api_response.status_code >= 400:\n",
    "        raise ValueError(\n",
    "            f\"API {api_response.url} returned an error status code: {api_response.status_code}. {api_response.json()=}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def validate_number_of_KNMI_files():\n",
    "    if len(dataset_files) > param_maximum_KNMI_files:\n",
    "        raise ValueError(\n",
    "            f\"{len(dataset_files)} KNMI files were found to download, but {param_maximum_KNMI_files=}.\"\n",
    "            f\"\\n The data was retrieved with the following parameters:\"\n",
    "            f\"\\n {param_start_date=} \\n {param_end_date=} \\n {param_interval_in_minutes=}\"\n",
    "            f\"\\n Increase {param_maximum_KNMI_files=}, decrease the time range, or increase the interval.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Strip any extra quotes\n",
    "init_complete = init_complete.replace(\"'\", \"\")\n",
    "init_complete = init_complete.replace('\"', \"\")\n",
    "if init_complete == \"Yes\":\n",
    "    print(\"Workflow configuration succesfull\")\n",
    "else:\n",
    "    print(\"Workflow configuration was not complete, exitting\")\n",
    "    import sys\n",
    "\n",
    "    sys.exit(1)\n",
    "\n",
    "# Notes:\n",
    "# Timestamps in iso8601\n",
    "# 2020-01-01T00:00+00:00\n",
    "\n",
    "# configure\n",
    "start_ts = param_start_date\n",
    "end_ts = param_end_date\n",
    "datasetName, datasetVersion, api_url, _ = conf_radars.get(param_radar.lower())\n",
    "params = {\n",
    "    \"datasetName\": datasetName,\n",
    "    \"datasetVersion\": datasetVersion,\n",
    "    \"maxKeys\": 10,\n",
    "    \"sorting\": \"asc\",\n",
    "    \"orderBy\": \"created\",\n",
    "    \"begin\": start_ts,\n",
    "    \"end\": end_ts,\n",
    "}\n",
    "# Request a response from the KNMI severs\n",
    "# Try the next page tokens\n",
    "dataset_files = []\n",
    "while True:\n",
    "    api_response = requests.get(\n",
    "        url=api_url,\n",
    "        headers={\"Authorization\": secret_key_knmi_api},\n",
    "        params=params,\n",
    "    )\n",
    "    validate_api_errors()\n",
    "\n",
    "    api_reponse_json = api_response.json()\n",
    "    dset_files = api_reponse_json.get(\"files\")\n",
    "\n",
    "    dset_files = [list(dset_file.values()) for dset_file in dset_files]\n",
    "    dataset_files += dset_files\n",
    "    nextPageToken = api_reponse_json.get(\"nextPageToken\")\n",
    "    if not nextPageToken:\n",
    "        break\n",
    "    else:\n",
    "        params.update({\"nextPageToken\": nextPageToken})\n",
    "\n",
    "# KNMI outputs per 5 minutes, per 15 is less of a heavy hit on downloads and processing\n",
    "# Quick and dirty way to only keep the 15 minute measurements.\n",
    "# Check API if we can filter for this on their end. If not fine\n",
    "filtered_list = []\n",
    "interval_list = list(range(0, 60, param_interval_in_minutes))\n",
    "for dataset_file in dataset_files:\n",
    "    minute = int(dataset_file[0].split(\"_\")[-1].split(\".\")[0][-2:])\n",
    "    if minute in interval_list:\n",
    "        filtered_list.append(dataset_file)\n",
    "\n",
    "dataset_files = filtered_list\n",
    "\n",
    "validate_number_of_KNMI_files()\n",
    "\n",
    "print(f\"Found {len(dataset_files)} files\")\n",
    "print(dataset_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbdd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download-KNMI\n",
    "##libraries\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Changes per 16-11-2023\n",
    "# Test if we are working with a one element nested list\n",
    "dataset_files\n",
    "n_files = len(dataset_files)\n",
    "print(f\"Starting download of {n_files} files.\")\n",
    "_, _, api_url, radar_code = conf_radars.get(param_radar.lower())\n",
    "knmi_pvol_paths = []\n",
    "idx = 1\n",
    "for dataset_file in dataset_files:\n",
    "    filename = dataset_file[0]\n",
    "    fname_parts = filename.split(\"_\")\n",
    "    fname_date_part = fname_parts[-1].split(\".\")[0]\n",
    "    year = fname_date_part[0:4]\n",
    "    month = fname_date_part[4:6]\n",
    "    day = fname_date_part[6:8]\n",
    "    p = Path(f\"{conf_local_knmi}/{radar_code}/{year}/{month}/{day}/{filename}\")\n",
    "    knmi_pvol_paths.append(\"{}\".format(str(p)))\n",
    "\n",
    "    if not p.exists():\n",
    "        print(f\"Downloading file {idx}/{n_files}\")\n",
    "        endpoint = f\"{api_url}/{filename}/url\"\n",
    "        get_file_response = requests.get(\n",
    "            endpoint, headers={\"Authorization\": secret_key_knmi_api}\n",
    "        )\n",
    "        download_url = get_file_response.json().get(\"temporaryDownloadUrl\")\n",
    "        dataset_file_response = requests.get(download_url)\n",
    "        p.parent.mkdir(parents=True, exist_ok=True)\n",
    "        p.write_bytes(dataset_file_response.content)\n",
    "    else:\n",
    "        print(f\"{p} already exists, skipping\")\n",
    "    idx += 1\n",
    "print(knmi_pvol_paths)\n",
    "print(\"Finished downloading files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf671ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNMI-to-ODIM-converter\n",
    "\"\"\"\n",
    "notes:\n",
    "Need to add this such that it can upload the PVOL From this stage\n",
    "Need to add option such that this can remove the PVOL files from this stage.\n",
    "Warning, with the removal of PVOL on this stage auto-bricks the VP / RBC gen\n",
    "We can introduce a flag check where RBC and VP check if PVOL 'needed' to be removed\n",
    "If that flag is met - abort, there 'shouldnt' be any INPUT files then.\n",
    "\"\"\"\n",
    "import subprocess\n",
    "import pathlib\n",
    "import h5py\n",
    "import json\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "# from typing import List, Object\n",
    "import math\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n",
    "        return True\n",
    "    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n",
    "        return False\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "\n",
    "class FileTranslatorFileTypeError(LookupError):\n",
    "    \"\"\"raise this when there's a filetype mismatch derived from h5 file\"\"\"\n",
    "\n",
    "\n",
    "def load_radar_db(radar_db_path):\n",
    "    \"\"\"Load and return the radar database\n",
    "\n",
    "    Output dict sample (wmo code is used as key):\n",
    "    {\n",
    "        11038: {'number': '1209', 'country': 'Austria', 'countryid': 'LOWM41', 'oldcountryid': 'OS41', 'wmocode': '11038', 'odimcode': 'atrau', 'location': 'Wien/Schwechat', 'status': '1', 'latitude': '48.074', 'longitude': '16.536', 'heightofstation': ' ', 'band': 'C', 'doppler': 'Y', 'polarization': 'D', 'maxrange': '224', 'startyear': '1978', 'heightantenna': '224', 'diametrantenna': ' ', 'beam': ' ', 'gain': ' ', 'frequency': '5.625', 'single_rrr': 'Y', 'composite_rrr': 'Y', 'wrwp': 'Y'},\n",
    "        11052: {'number': '1210', 'country': 'Austria', 'countryid': 'LOWM43', 'oldcountryid': 'OS43', 'wmocode': '11052', 'odimcode': 'atfel', 'location': 'Salzburg/Feldkirchen', 'status': '1', 'latitude': '48.065', 'longitude': '13.062', 'heightofstation': ' ', 'band': 'C', 'doppler': 'Y', 'polarization': 'D', 'maxrange': '224', 'startyear': '1992', 'heightantenna': '581', 'diametrantenna': ' ', 'beam': ' ', 'gain': ' ', 'frequency': '5.6', 'single_rrr': 'Y', 'composite_rrr': ' ', 'wrwp': ' '},\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(radar_db_path, mode=\"r\") as f:\n",
    "        radar_db_json = json.load(f)\n",
    "    radar_db = {}\n",
    "    # Reorder list to a usable dict with sub dicts which we can search with wmo codes\n",
    "    for radar_dict in radar_db_json:\n",
    "        try:\n",
    "            wmo_code = int(radar_dict.get(\"wmocode\"))\n",
    "            radar_db.update({wmo_code: radar_dict})\n",
    "        except Exception:  # Happens when there is for ex. no wmo code.\n",
    "            pass\n",
    "    return radar_db\n",
    "\n",
    "\n",
    "def translate_wmo_odim(radar_db, wmo_code):\n",
    "    \"\"\" \"\"\"\n",
    "    if not isinstance(wmo_code, int):\n",
    "        raise ValueError(\"Expecting a wmo_code [int]\")\n",
    "    else:\n",
    "        pass\n",
    "    odim_code = (\n",
    "        radar_db.get(wmo_code).get(\"odimcode\").upper().strip()\n",
    "    )  # Apparently, people sometimes forget to remove whitespace..\n",
    "    return odim_code\n",
    "\n",
    "\n",
    "def extract_wmo_code(in_path):\n",
    "    with h5py.File(in_path, mode=\"r\") as f:\n",
    "        # DWD Specific\n",
    "        # Main attributes\n",
    "        what = f[\"what\"].attrs\n",
    "        # Source block\n",
    "        source = what.get(\"source\")\n",
    "        source = source.decode(\"utf-8\")\n",
    "        # Determine if we are dealing with a WMO code or with an ODIM code set\n",
    "        # Example from Germany where source block is set as WMO\n",
    "        # what/source: \"WMO:10103\"\n",
    "        # Example from The Netherlands where source block is set as a combination of ODIM and various codes\n",
    "        # what/source: RAD:NL52,NOD:nlhrw,PLC:Herwijnen\n",
    "        source_list = source.split(sep=\",\")\n",
    "    wmo_code = [string for string in source_list if \"WMO\" in string]\n",
    "    # Determine if we had exactly one WMO hit\n",
    "    if len(wmo_code) == 1:\n",
    "        wmo_code = wmo_code[0]\n",
    "        wmo_code = wmo_code.replace(\"WMO:\", \"\")\n",
    "    # No wmo code found, most likeley dealing with a dutch radar\n",
    "    elif len(wmo_code) == 0:\n",
    "        rad_str = [string for string in source_list if \"RAD\" in string]\n",
    "\n",
    "        if len(rad_str) == 1:\n",
    "            rad_str = rad_str[0]\n",
    "        else:\n",
    "            print(\n",
    "                \"Something went wrong with determining the rad_str and it wasnt WMO either, exitting\"\n",
    "            )\n",
    "            sys.exit(1)\n",
    "        # Split the rad_str\n",
    "        rad_str_split = rad_str.split(\":\")\n",
    "        # [0] = RAD, [1] = rad code\n",
    "        rad_code = rad_str_split[1]\n",
    "\n",
    "        rad_codes = {\"NL52\": \"6356\", \"NL51\": \"6234\", \"NL50\": \"6260\"}\n",
    "\n",
    "        wmo_code = rad_codes.get(rad_code)\n",
    "    return int(wmo_code)\n",
    "\n",
    "\n",
    "def translate_knmi_filename(in_path_h5):\n",
    "    wmo_code = extract_wmo_code(in_path_h5)\n",
    "    odim_code = translate_wmo_odim(radar_db, wmo_code)\n",
    "    with h5py.File(in_path_h5, mode=\"r\") as f:\n",
    "        what = f[\"what\"].attrs\n",
    "        # Date block\n",
    "        date = what.get(\"date\")\n",
    "        date = date.decode(\"utf-8\")\n",
    "        # Time block\n",
    "        time = what.get(\"time\")\n",
    "        # time = f['dataset1/what'].attrs['endtime']\n",
    "        time = time.decode(\"utf-8\")\n",
    "        hh = time[:2]\n",
    "        mm = time[2:4]\n",
    "        ss = time[4:]\n",
    "        time = time[:-2]  # Do not include seconds\n",
    "        # File type\n",
    "        filetype = what.get(\"object\")\n",
    "        filetype = filetype.decode(\"utf-8\")\n",
    "        if filetype != \"PVOL\":\n",
    "            raise FileTranslatorFileTypeError(\"File type was NOT pvol\")\n",
    "    name = [\n",
    "        odim_code,\n",
    "        filetype.lower(),\n",
    "        date + \"T\" + time,\n",
    "        str(wmo_code) + \".h5\",\n",
    "    ]\n",
    "    ibed_fname = \"_\".join(name)\n",
    "    return ibed_fname\n",
    "\n",
    "\n",
    "def knmi_to_odim(in_fpath, out_fpath):\n",
    "    \"\"\"\n",
    "    Converter usage:\n",
    "    Usage: KNMI_vol_h5_to_ODIM_h5 ODIM_file.h5 KNMI_input_file.h5\n",
    "\n",
    "    Returns out_fpath and returncode\n",
    "    \"\"\"\n",
    "    converter = \"/opt/radar/vol2bird/bin/./KNMI_vol_h5_to_ODIM_h5\"\n",
    "    command = [converter, out_fpath, in_fpath]\n",
    "    proc = subprocess.run(command, stderr=subprocess.PIPE)\n",
    "    output = proc.stderr.decode(\"utf-8\")\n",
    "    returncode = int(proc.returncode)\n",
    "    return (out_fpath, returncode, output)\n",
    "\n",
    "\n",
    "def get_pvol_storage_path(relative_path: str = \"\") -> str:\n",
    "    if param_public_minio_data:\n",
    "        return (\n",
    "            pathlib.Path(conf_minio_public_root_prefix)\n",
    "            .joinpath(conf_minio_tutorial_prefix)\n",
    "            .joinpath(conf_pvol_output_prefix)\n",
    "            .joinpath(relative_path)\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            pathlib.Path(conf_minio_tutorial_prefix)\n",
    "            .joinpath(conf_user_directory + param_user_number)\n",
    "            .joinpath(conf_pvol_output_prefix)\n",
    "            .joinpath(relative_path)\n",
    "        )\n",
    "\n",
    "\n",
    "print(f\"{knmi_pvol_paths=}\")\n",
    "odim_pvol_paths = []\n",
    "radar_db = load_radar_db(conf_local_radar_db)\n",
    "for knmi_path in knmi_pvol_paths:\n",
    "    out_path_pvol_odim = pathlib.Path(knmi_path.replace(\"knmi\", \"odim\"))\n",
    "    print(f\"{knmi_path=}\")\n",
    "    print(f\"{out_path_pvol_odim=}\")\n",
    "    if not out_path_pvol_odim.parent.exists():\n",
    "        out_path_pvol_odim.parent.mkdir(parents=True, exist_ok=False)\n",
    "    converter_results = knmi_to_odim(\n",
    "        in_fpath=str(knmi_path), out_fpath=str(out_path_pvol_odim)\n",
    "    )\n",
    "    print(f\"{converter_results=}\")\n",
    "    if param_clean_knmi_input:\n",
    "        pathlib.Path(knmi_path).unlink()\n",
    "        if not any(pathlib.Path(knmi_path).parent.iterdir()):\n",
    "            pathlib.Path(knmi_path).parent.rmdir()\n",
    "    # Determine name for our convention\n",
    "    ibed_pvol_name = translate_knmi_filename(in_path_h5=out_path_pvol_odim)\n",
    "    out_path_pvol_odim_tce = pathlib.Path(out_path_pvol_odim).parent.joinpath(\n",
    "        ibed_pvol_name\n",
    "    )\n",
    "    shutil.move(src=out_path_pvol_odim, dst=out_path_pvol_odim_tce)\n",
    "    odim_pvol_paths.append(out_path_pvol_odim_tce)\n",
    "\n",
    "print(f\"{odim_pvol_paths=}\")\n",
    "if str2bool(param_upload_results):\n",
    "    # Minio version\n",
    "    from minio import Minio\n",
    "\n",
    "    minioClient = Minio(\n",
    "        endpoint=conf_minio_endpoint,\n",
    "        access_key=secret_minio_access_key,\n",
    "        secret_key=secret_minio_secret_key,\n",
    "        secure=True,\n",
    "    )\n",
    "    print(f\"Uploading results to {get_pvol_storage_path()}\")\n",
    "    for odim_pvol_path in odim_pvol_paths:\n",
    "        odim_pvol_path = pathlib.Path(odim_pvol_path)\n",
    "        local_pvol_storage = pathlib.Path(conf_local_odim)\n",
    "        relative_path = odim_pvol_path.relative_to(local_pvol_storage)\n",
    "        remote_odim_pvol_path = get_pvol_storage_path(relative_path)\n",
    "        # check if this exists\n",
    "        exists = False\n",
    "        try:\n",
    "            _ = minioClient.stat_object(\n",
    "                bucket=(\n",
    "                    conf_minio_public_bucket_name\n",
    "                    if param_public_minio_data\n",
    "                    else conf_minio_user_bucket_name\n",
    "                ),\n",
    "                prefix=remote_odim_pvol_path.as_posix(),\n",
    "            )\n",
    "            exists = True\n",
    "        except:\n",
    "            pass\n",
    "        if not exists:\n",
    "            print(f\"Uploading {odim_pvol_path} to {remote_odim_pvol_path}\")\n",
    "            with open(odim_pvol_path, mode=\"rb\") as file_data:\n",
    "                file_stat = os.stat(odim_pvol_path)\n",
    "                minioClient.put_object(\n",
    "                    bucket_name=(\n",
    "                        conf_minio_public_bucket_name\n",
    "                        if param_public_minio_data\n",
    "                        else conf_minio_user_bucket_name\n",
    "                    ),\n",
    "                    object_name=remote_odim_pvol_path.as_posix(),\n",
    "                    data=file_data,\n",
    "                    length=file_stat.st_size,\n",
    "                )\n",
    "        else:\n",
    "            print(f\"{remote_odim_pvol_path} exists, skipping \")\n",
    "    print(\"Finished uploading results\")\n",
    "# cast to string to not break json serializer\n",
    "odim_pvol_paths = [path.as_posix() for path in odim_pvol_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334de446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PVOL-VP-converter\n",
    "import pandas as pd\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n",
    "        return True\n",
    "    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n",
    "        return False\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "\n",
    "def load_radar_db(radar_db_path):\n",
    "    \"\"\"Load and return the radar database\n",
    "    Output dict sample (wmo code is used as key):\n",
    "    {\n",
    "        11038: {'number': '1209', 'country': 'Austria', 'countryid': 'LOWM41', 'oldcountryid': 'OS41', 'wmocode': '11038', 'odimcode': 'atrau', 'location': 'Wien/Schwechat', 'status': '1', 'latitude': '48.074', 'longitude': '16.536', 'heightofstation': ' ', 'band': 'C', 'doppler': 'Y', 'polarization': 'D', 'maxrange': '224', 'startyear': '1978', 'heightantenna': '224', 'diametrantenna': ' ', 'beam': ' ', 'gain': ' ', 'frequency': '5.625', 'single_rrr': 'Y', 'composite_rrr': 'Y', 'wrwp': 'Y'},\n",
    "        11052: {'number': '1210', 'country': 'Austria', 'countryid': 'LOWM43', 'oldcountryid': 'OS43', 'wmocode': '11052', 'odimcode': 'atfel', 'location': 'Salzburg/Feldkirchen', 'status': '1', 'latitude': '48.065', 'longitude': '13.062', 'heightofstation': ' ', 'band': 'C', 'doppler': 'Y', 'polarization': 'D', 'maxrange': '224', 'startyear': '1992', 'heightantenna': '581', 'diametrantenna': ' ', 'beam': ' ', 'gain': ' ', 'frequency': '5.6', 'single_rrr': 'Y', 'composite_rrr': ' ', 'wrwp': ' '},\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(radar_db_path, mode=\"r\") as f:\n",
    "        radar_db_json = json.load(f)\n",
    "    radar_db = {}\n",
    "    # Reorder list to a usable dict with sub dicts which we can search with wmo codes\n",
    "    for radar_dict in radar_db_json:\n",
    "        try:\n",
    "            wmo_code = int(radar_dict.get(\"wmocode\"))\n",
    "            radar_db.update({wmo_code: radar_dict})\n",
    "        except Exception:  # Happens when there is for ex. no wmo code.\n",
    "            pass\n",
    "    return radar_db\n",
    "\n",
    "\n",
    "def translate_wmo_odim(radar_db, wmo_code):\n",
    "    \"\"\"\"\"\"\n",
    "    # class FileTranslatorFileTypeError(LookupError):\n",
    "    #    '''raise this when there's a filetype mismatch derived from h5 file'''\n",
    "    if not isinstance(wmo_code, int):\n",
    "        raise ValueError(\"Expecting a wmo_code [int]\")\n",
    "    else:\n",
    "        pass\n",
    "    odim_code = (\n",
    "        radar_db.get(wmo_code).get(\"odimcode\").upper().strip()\n",
    "    )  # Apparently, people sometimes forget to remove whitespace..\n",
    "    return odim_code\n",
    "\n",
    "\n",
    "def extract_wmo_code(in_path):\n",
    "    with h5py.File(in_path, \"r\") as f:\n",
    "        # DWD Specific\n",
    "        # Main attributes\n",
    "        what = f[\"what\"].attrs\n",
    "        # Source block\n",
    "        source = what.get(\"source\")\n",
    "        source = source.decode(\"utf-8\")\n",
    "        # Determine if we are dealing with a WMO code or with an ODIM code set\n",
    "        # Example from Germany where source block is set as WMO\n",
    "        # what/source: \"WMO:10103\"\n",
    "        # Example from The Netherlands where source block is set as a combination of ODIM and various codes\n",
    "        # what/source: RAD:NL52,NOD:nlhrw,PLC:Herwijnen\n",
    "        source_list = source.split(sep=\",\")\n",
    "    wmo_code = [string for string in source_list if \"WMO\" in string]\n",
    "    # Determine if we had exactly one WMO hit\n",
    "    if len(wmo_code) == 1:\n",
    "        wmo_code = wmo_code[0]\n",
    "        wmo_code = wmo_code.replace(\"WMO:\", \"\")\n",
    "    # No wmo code found, most likeley dealing with a dutch radar\n",
    "    elif len(wmo_code) == 0:\n",
    "        rad_str = [string for string in source_list if \"RAD\" in string]\n",
    "        if len(rad_str) == 1:\n",
    "            rad_str = rad_str[0]\n",
    "        else:\n",
    "            print(\n",
    "                \"Something went wrong with determining the rad_str and it wasnt WMO either, exiting\"\n",
    "            )\n",
    "            sys.exit(1)\n",
    "        # Split the rad_str\n",
    "        rad_str_split = rad_str.split(\":\")\n",
    "        # [0] = RAD, [1] = rad code\n",
    "        rad_code = rad_str_split[1]\n",
    "        rad_codes = {\"NL52\": \"6356\", \"NL51\": \"6234\", \"NL50\": \"6260\"}\n",
    "        wmo_code = rad_codes.get(rad_code)\n",
    "    return int(wmo_code)\n",
    "\n",
    "\n",
    "def vol2bird(\n",
    "    in_file,\n",
    "    out_dir,\n",
    "    radar_db,\n",
    "    add_version=True,\n",
    "    add_sector=False,\n",
    "    overwrite=False,\n",
    "):\n",
    "    # Construct output file\n",
    "    date_regex = \"([0-9]{8})\"\n",
    "    if add_version == True:\n",
    "        version = \"v0-3-20\"\n",
    "        suffix = pathlib.Path(in_file).suffix\n",
    "        in_file_name = pathlib.Path(in_file).name\n",
    "        in_file_stem = pathlib.Path(in_file_name).stem\n",
    "        #\n",
    "        out_file_name = in_file_stem.replace(\"pvol\", \"vp\")\n",
    "        out_file_name = \"_\".join([out_file_name, version]) + suffix\n",
    "        # odim = odim_code(out_file_name)\n",
    "        wmo = extract_wmo_code(in_file)\n",
    "        odim = translate_wmo_odim(radar_db, wmo)\n",
    "        datetime = pd.to_datetime(re.search(date_regex, out_file_name)[0])\n",
    "        ibed_path = \"/\".join(\n",
    "            [\n",
    "                odim[:2],\n",
    "                odim[2:],\n",
    "                str(datetime.year),\n",
    "                str(datetime.month).zfill(2),\n",
    "                str(datetime.day).zfill(2),\n",
    "            ]\n",
    "        )\n",
    "        # check if we need to make this dir\n",
    "        out_file = \"/\".join([out_dir, ibed_path, out_file_name])\n",
    "        out_file_dir = pathlib.Path(out_file).parent\n",
    "        if not out_file_dir.exists():\n",
    "            out_file_dir.mkdir(parents=True)\n",
    "\n",
    "    process = False\n",
    "    if not overwrite:\n",
    "        if not pathlib.Path(out_file).exists():\n",
    "            process = True\n",
    "            print(f\"Not processing, overwrite is set to {overwrite}\")\n",
    "    else:\n",
    "        process = True\n",
    "\n",
    "    if process:\n",
    "        command = [\"vol2bird\", in_file, out_file]\n",
    "        result = subprocess.run(\n",
    "            command, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT\n",
    "        )\n",
    "    return [in_file, out_file]\n",
    "\n",
    "\n",
    "def get_vp_storage_path(relative_path: str = \"\") -> str:\n",
    "    if param_public_minio_data:\n",
    "        return (\n",
    "            pathlib.Path(conf_minio_public_root_prefix)\n",
    "            .joinpath(conf_minio_tutorial_prefix)\n",
    "            .joinpath(conf_vp_output_prefix)\n",
    "            .joinpath(relative_path)\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            pathlib.Path(conf_minio_tutorial_prefix)\n",
    "            .joinpath(conf_user_directory + param_user_number)\n",
    "            .joinpath(conf_vp_output_prefix)\n",
    "            .joinpath(relative_path)\n",
    "        )\n",
    "\n",
    "\n",
    "vertical_profile_paths = []\n",
    "radar_db = load_radar_db(conf_local_radar_db)\n",
    "# cast back to pathlib after deserializing\n",
    "odim_pvol_paths = [pathlib.Path(path) for path in odim_pvol_paths]\n",
    "for odim_pvol_path in odim_pvol_paths:\n",
    "    pvol_path, vp_path = vol2bird(\n",
    "        odim_pvol_path, conf_local_vp, radar_db, overwrite=False\n",
    "    )\n",
    "    vertical_profile_paths.append(vp_path)\n",
    "print(vertical_profile_paths)\n",
    "\n",
    "if str2bool(param_clean_pvol_output):\n",
    "    print(\"Removing PVOL output from local storage\")\n",
    "    for pvol_path in odim_pvol_paths:\n",
    "        pathlib.Path(pvol_path).unlink()\n",
    "        if not any(pathlib.Path(pvol_path).parent.iterdir()):\n",
    "            pathlib.Path(pvol_path).parent.rmdir()\n",
    "\n",
    "if str2bool(param_upload_results):\n",
    "    # Minio version\n",
    "    from minio import Minio\n",
    "\n",
    "    minioClient = Minio(\n",
    "        endpoint=conf_minio_endpoint,\n",
    "        access_key=secret_minio_access_key,\n",
    "        secret_key=secret_minio_secret_key,\n",
    "        secure=True,\n",
    "    )\n",
    "    print(f\"Uploading results to {get_vp_storage_path()}\")\n",
    "    for vp_path in vertical_profile_paths:\n",
    "        vp_path = pathlib.Path(vp_path)\n",
    "        local_vp_storage = pathlib.Path(conf_local_vp)\n",
    "        relative_path = vp_path.relative_to(local_vp_storage)\n",
    "        remote_vp_path = get_vp_storage_path(relative_path)\n",
    "        # check if this exists\n",
    "        exists = False\n",
    "        try:\n",
    "            _ = minioClient.stat_object(\n",
    "                bucket=(\n",
    "                    conf_minio_public_bucket_name\n",
    "                    if param_public_minio_data\n",
    "                    else conf_minio_user_bucket_name\n",
    "                ),\n",
    "                prefix=remote_vp_path.as_posix(),\n",
    "            )\n",
    "            exists = True\n",
    "        except:\n",
    "            pass\n",
    "        if not exists:\n",
    "            print(f\"Uploading {vp_path} to {remote_vp_path}\")\n",
    "            with open(vp_path, mode=\"rb\") as file_data:\n",
    "                file_stat = os.stat(vp_path)\n",
    "                minioClient.put_object(\n",
    "                    bucket_name=(\n",
    "                        conf_minio_public_bucket_name\n",
    "                        if param_public_minio_data\n",
    "                        else conf_minio_user_bucket_name\n",
    "                    ),\n",
    "                    object_name=remote_vp_path.as_posix(),\n",
    "                    data=file_data,\n",
    "                    length=file_stat.st_size,\n",
    "                )\n",
    "        else:\n",
    "            print(f\"{remote_vp_path} exists, skipping \")\n",
    "    print(\"Finished uploading results\")\n",
    "if str2bool(param_clean_vp_output):\n",
    "    print(\"Removing VP output from local storage\")\n",
    "    for vp_path in vertical_profile_paths:\n",
    "        pathlib.Path(vp_path).unlink()\n",
    "        if not any(pathlib.Path(vp_path).parent.iterdir()):\n",
    "            pathlib.Path(vp_path).parent.rmdir()\n",
    "\n",
    "PVOL_VP_converter_complete = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve PVOL DEPRECATED\n",
    "from minio import Minio\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "if PVOL_VP_converter_complete:\n",
    "    print(\"PVOL-VP-converter successfull\")\n",
    "else:\n",
    "    print(\"PVOL-VP-converter was not complete, exitting\")\n",
    "    import sys\n",
    "\n",
    "    sys.exit(1)\n",
    "\n",
    "#\n",
    "minioClient = Minio(\n",
    "    endpoint=conf_minio_endpoint,\n",
    "    access_key=secret_minio_access_key,\n",
    "    secret_key=secret_minio_secret_key,\n",
    "    secure=True,\n",
    ")\n",
    "\n",
    "# dtype = \"pvol\"\n",
    "# country = \"NL\"\n",
    "# radar = \"DHL\"\n",
    "# year = \"2023\"\n",
    "# month = \"12\"\n",
    "# day = \"31\"\n",
    "recursive = True\n",
    "\n",
    "if param_dtype.lower() in [\"pvol\", \"polar volume\", \"polarvolume\"]:\n",
    "    search_prefix = f\"{conf_minio_tutorial_prefix}/{conf_user_directory+param_user_number}/{conf_pvol_output_prefix}/{param_country}/{param_radar}/{param_year}/{param_month}/{param_day}\"\n",
    "elif param_dtype.lower() in [\"vp\", \"vertical profile\", \"verticalprofile\"]:\n",
    "    search_prefix = f\"{conf_minio_tutorial_prefix}/{conf_user_directory+param_user_number}/{conf_vp_output_prefix}/{param_country}/{param_radar}/{param_year}/{param_month}/{param_day}\"\n",
    "else:\n",
    "    print(f\"{param_dtype} not understood\")\n",
    "    sys.exit(1)\n",
    "print(f\"{search_prefix=}\")\n",
    "# To be implemented:\n",
    "# The below works, but we can use this for the filtering from parameters at some point.\n",
    "# This shoud be done after the demo version.\n",
    "# start_after_prefix=f'{conf_minio_tutorial_prefix}/{conf_pvol_output_prefix}/{country}/{radar}/{year}/{month}/{day}/{country}{radar}_{dtype}_{year}{month}{day}T2200_6234.h5'\n",
    "# print(f\"{start_after_prefix=}\")\n",
    "# objects = minioClient.list_objects(bucket_name=conf_minio_user_bucket_name,\n",
    "#                                   prefix=search_prefix,\n",
    "#                                   recursive=recursive,\n",
    "#                                   start_after=start_after_prefix\n",
    "#                                  )\n",
    "objects = minioClient.list_objects(\n",
    "    bucket_name=(\n",
    "        conf_minio_public_bucket_name\n",
    "        if param_public_minio_data\n",
    "        else conf_minio_user_bucket_name\n",
    "    ),\n",
    "    prefix=search_prefix,\n",
    "    recursive=recursive,\n",
    ")\n",
    "local_file_paths = []\n",
    "for obj in objects:\n",
    "    obj_path = pathlib.Path(obj._object_name)\n",
    "    local_file_path = f\"{conf_local_visualization_input}/{obj_path.name}\"\n",
    "    local_file_paths.append(local_file_path)\n",
    "    print(f\"Downloading {obj._object_name} to {local_file_path}\")\n",
    "    minioClient.fget_object(\n",
    "        bucket_name=obj._bucket_name,\n",
    "        object_name=obj._object_name,\n",
    "        file_path=local_file_path,\n",
    "    )\n",
    "    local_file_paths.append(local_file_path)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d17296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 PVOL Downloader\n",
    "# ---\n",
    "# NaaVRE:\n",
    "#  cell:\n",
    "#   outputs:\n",
    "#    - local_pvol_paths: List\n",
    "# ...\n",
    "\n",
    "# Code analyzer fix block. Assign empty strings to variables that should not be picked up by the analyzer.\n",
    "minioClient = \"\"\n",
    "\n",
    "# libraries\n",
    "from minio import Minio\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "\n",
    "# functions\n",
    "def get_pvol_storage_path(relative_path: str = \"\") -> str:\n",
    "    if param_public_minio_data:\n",
    "        return (\n",
    "            pathlib.Path(conf_minio_public_root_prefix)\n",
    "            .joinpath(conf_minio_tutorial_prefix)\n",
    "            .joinpath(conf_pvol_output_prefix)\n",
    "            .joinpath(relative_path)\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            pathlib.Path(conf_minio_tutorial_prefix)\n",
    "            .joinpath(conf_user_directory + param_user_number)\n",
    "            .joinpath(conf_pvol_output_prefix)\n",
    "            .joinpath(relative_path)\n",
    "        )\n",
    "\n",
    "\n",
    "# main\n",
    "psd = pd.to_datetime(param_start_date)\n",
    "ped = pd.to_datetime(param_end_date)\n",
    "minioClient = Minio(\n",
    "    endpoint=conf_minio_endpoint,\n",
    "    access_key=secret_minio_access_key,\n",
    "    secret_key=secret_minio_secret_key,\n",
    "    secure=True,\n",
    ")\n",
    "\n",
    "download_objs = []\n",
    "\n",
    "\n",
    "# grab psd and ped, rework them to include a start_after prefix\n",
    "psd_prefix = f\"{get_pvol_storage_path()}/NL/{param_radar}/{psd.year}/{psd.month:02}/{psd.day:02}\"\n",
    "print(f\"Parsing first prefix: {psd_prefix}\")\n",
    "psd_start_after_prefix = f\"{psd_prefix}/NL{param_radar}_pvol_{psd.year}{psd.month:02}{psd.day:02}T{psd.hour:02}{psd.minute:02}\"\n",
    "print(f\"Building an start after prefix: {psd_start_after_prefix}\")\n",
    "psd_prefix_objs = minioClient.list_objects(\n",
    "    bucket_name=(\n",
    "        conf_minio_public_bucket_name\n",
    "        if param_public_minio_data\n",
    "        else conf_minio_user_bucket_name\n",
    "    ),\n",
    "    prefix=psd_prefix,\n",
    "    start_after=psd_start_after_prefix,\n",
    "    recursive=True,\n",
    ")\n",
    "psd_prefix_objs = list(psd_prefix_objs)\n",
    "print(f\"{psd_prefix_objs=}\")\n",
    "download_objs += psd_prefix_objs\n",
    "\n",
    "# Now we need a middle prefix download. The prefixes that are between psd and ped.\n",
    "print(f\"Determining middle prefixes...\")\n",
    "drange = pd.date_range(start=psd, end=ped, freq=\"5 min\")\n",
    "date_prefix_list = [\n",
    "    f\"{dstamp.year}/{dstamp.month:02}/{dstamp.day:02}\" for dstamp in drange\n",
    "]\n",
    "unique_date_prefix_list = list(set(date_prefix_list))\n",
    "# sort the dates\n",
    "unique_date_prefix_list.sort()\n",
    "# remove first prefix, we evaluate that differently\n",
    "middle_prefixes = unique_date_prefix_list[1:-1]\n",
    "# Now add the correct country, radar to the date prefixes\n",
    "middle_prefixes = [\n",
    "    f\"{get_pvol_storage_path()}/NL/{param_radar}/{middle_prefix}\"\n",
    "    for middle_prefix in middle_prefixes\n",
    "]\n",
    "print(f\"Parsing {len(middle_prefixes)} middle prefixes.\")\n",
    "for middle_prefix in middle_prefixes:\n",
    "    print(f\"Downloading {middle_prefix}\")\n",
    "    middle_prefix_objs = minioClient.list_objects(\n",
    "        bucket_name=(\n",
    "            conf_minio_public_bucket_name\n",
    "            if param_public_minio_data\n",
    "            else conf_minio_user_bucket_name\n",
    "        ),\n",
    "        prefix=middle_prefix,\n",
    "        recursive=True,\n",
    "    )\n",
    "    download_objs += list(middle_prefix_objs)\n",
    "\n",
    "\n",
    "# For PED we need a 'until'.\n",
    "# So, we need to determine which part of the list of the final prefix we require.\n",
    "# in essence, we need a ped_until_prefix.\n",
    "ped_prefix = f\"{get_pvol_storage_path()}/NL/{param_radar}/{ped.year}/{ped.month:02}/{ped.day:02}\"\n",
    "\n",
    "ped_until_prefix = f\"{ped_prefix}/NL{param_radar}_pvol_{ped.year}{ped.month:02}{ped.day:02}T{ped.hour:02}{ped.minute:02}\"\n",
    "print(f\"Parsing last prefix:{ped_prefix}\")\n",
    "ped_until_datetimestr = (\n",
    "    f\"{ped.year}{ped.month:02}{ped.day:02}T{ped.hour:02}{ped.minute:02}\"\n",
    ")\n",
    "ped_until_timestamp = pd.to_datetime(ped_until_datetimestr)\n",
    "print(f\"Building an end timestamp for object filtering: {ped_until_timestamp}\")\n",
    "ped_prefix_objs = minioClient.list_objects(\n",
    "    bucket_name=(\n",
    "        conf_minio_public_bucket_name\n",
    "        if param_public_minio_data\n",
    "        else conf_minio_user_bucket_name\n",
    "    ),\n",
    "    prefix=ped_prefix,\n",
    "    recursive=True,\n",
    ")\n",
    "print(f\"Filtering last prefix objects on timestamps\")\n",
    "ped_prefix_objs = list(ped_prefix_objs)\n",
    "print(f\"{ped_prefix_objs=}\")\n",
    "_ped_prefix_objs = []\n",
    "for obj in ped_prefix_objs:\n",
    "    fpath = pathlib.Path(obj._object_name)\n",
    "    fname = fpath.name\n",
    "    corad, dtype, datetimestr, radcode_suffix = fname.split(\"_\")\n",
    "    timestamp = pd.to_datetime(datetimestr)\n",
    "    if timestamp <= ped_until_timestamp:\n",
    "        _ped_prefix_objs.append(obj)\n",
    "        download_objs.append(obj)\n",
    "\n",
    "ped_prefix_objs = _ped_prefix_objs\n",
    "\n",
    "# Ensure that download objects are not duplicated in case of single prefix searches\n",
    "if psd_prefix == ped_prefix:\n",
    "    # same prefix\n",
    "    print(\"Single prefix filtering\")\n",
    "    psd_object_names = [\n",
    "        psd_prefix_obj._object_name for psd_prefix_obj in psd_prefix_objs\n",
    "    ]\n",
    "    print(f\"{psd_object_names=}\")\n",
    "    ped_object_names = [\n",
    "        ped_prefix_obj._object_name for ped_prefix_obj in ped_prefix_objs\n",
    "    ]\n",
    "    print(f\"{ped_object_names=}\")\n",
    "    intersect_object_names = [\n",
    "        obj_name\n",
    "        for obj_name in psd_object_names\n",
    "        if obj_name in ped_object_names\n",
    "    ]\n",
    "    print(f\"{intersect_object_names=}\")\n",
    "    # Reset download_objs list\n",
    "    download_objs = []\n",
    "    for psd_prefix_obj in psd_prefix_objs:\n",
    "        if psd_prefix_obj._object_name in intersect_object_names:\n",
    "            download_objs.append(psd_prefix_obj)\n",
    "\n",
    "\n",
    "local_pvol_paths = []\n",
    "for obj in download_objs:\n",
    "    obj_path = pathlib.Path(obj._object_name)\n",
    "    if param_public_minio_data:\n",
    "        lab, workshop, dtype, country, radar, year, month, day, filename = (\n",
    "            obj_path.parts\n",
    "        )\n",
    "    else:\n",
    "        workshop, uname, dtype, country, radar, year, month, day, filename = (\n",
    "            obj_path.parts\n",
    "        )\n",
    "    local_pvol_path = (\n",
    "        f\"{conf_local_odim}/{country}/{radar}/{year}/{month}/{day}/{filename}\"\n",
    "    )\n",
    "    print(local_pvol_path)\n",
    "    minioClient.fget_object(\n",
    "        bucket_name=obj._bucket_name,\n",
    "        object_name=obj._object_name,\n",
    "        file_path=local_pvol_path,\n",
    "    )\n",
    "    local_pvol_paths.append(local_pvol_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f81cb-0527-478b-9a3a-b7d69091c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 VP Downloader\n",
    "# ---\n",
    "# NaaVRE:\n",
    "#  cell:\n",
    "#   outputs:\n",
    "#    - vp_paths: List\n",
    "# ...\n",
    "\n",
    "# Code analyzer fix block. Assign empty strings to variables that should not be picked up by the analyzer.\n",
    "minioClient = \"\"\n",
    "\n",
    "# libraries\n",
    "from minio import Minio\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "# functions\n",
    "\n",
    "# main\n",
    "psd = pd.to_datetime(param_start_date)\n",
    "ped = pd.to_datetime(param_end_date)\n",
    "minioClient = Minio(\n",
    "    endpoint=conf_minio_endpoint,\n",
    "    access_key=secret_minio_access_key,\n",
    "    secret_key=secret_minio_secret_key,\n",
    "    secure=True,\n",
    ")\n",
    "\n",
    "download_objs = []\n",
    "\n",
    "# grab psd and ped, rework them to include a start_after prefix\n",
    "psd_prefix = f\"bwijers1@gmail.com/vp/NL/{param_radar}/{psd.year}/{psd.month:02}/{psd.day:02}\"\n",
    "psd_start_after_prefix = f\"{psd_prefix}/NL{param_radar}_vp_{psd.year}{psd.month:02}{psd.day:02}T{psd.hour:02}{psd.minute:02}\"\n",
    "psd_prefix_objs = minioClient.list_objects(\n",
    "    bucket_name=conf_minio_user_bucket_name,\n",
    "    prefix=psd_prefix,\n",
    "    start_after=psd_start_after_prefix,\n",
    "    recursive=True,\n",
    ")\n",
    "download_objs += list(psd_prefix_objs)\n",
    "\n",
    "# For PED we need a 'until'.\n",
    "# So, we need to determine which part of the list of the final prefix we require.\n",
    "# in essence, we need a ped_until_prefix.\n",
    "ped_prefix = f\"bwijers1@gmail.com/vp/NL/{param_radar}/{ped.year}/{ped.month:02}/{ped.day:02}\"\n",
    "ped_until_prefix = f\"{ped_prefix}/NL{param_radar}_vp_{ped.year}{ped.month:02}{ped.day:02}T{ped.hour:02}{ped.minute:02}\"\n",
    "ped_until_datetimestr = (\n",
    "    f\"{ped.year}{ped.month:02}{ped.day:02}T{ped.hour:02}{ped.minute:02}\"\n",
    ")\n",
    "ped_until_timestamp = pd.to_datetime(ped_until_datetimestr)\n",
    "ped_prefix_objs = minioClient.list_objects(\n",
    "    bucket_name=conf_minio_user_bucket_name, prefix=ped_prefix, recursive=True\n",
    ")\n",
    "ped_prefix_objs = list(ped_prefix_objs)\n",
    "for obj in ped_prefix_objs:\n",
    "    fpath = pathlib.Path(obj._object_name)\n",
    "    fname = fpath.name\n",
    "    corad, dtype, datetimestr, radcode, v2b_version_suffix = fname.split(\"_\")\n",
    "    timestamp = pd.to_datetime(datetimestr)\n",
    "    if timestamp <= ped_until_timestamp:\n",
    "        download_objs.append(obj)\n",
    "\n",
    "vp_paths = []\n",
    "for obj in download_objs:\n",
    "    obj_path = pathlib.Path(obj._object_name)\n",
    "    uname, dtype, country, radar, year, month, day, filename = obj_path.parts\n",
    "    local_vp_path = (\n",
    "        f\"{conf_local_vp}/{country}/{radar}/{year}/{month}/{day}/{filename}\"\n",
    "    )\n",
    "    print(local_vp_path)\n",
    "    minioClient.fget_object(\n",
    "        bucket_name=obj._bucket_name,\n",
    "        object_name=obj._object_name,\n",
    "        file_path=local_vp_path,\n",
    "    )\n",
    "    vp_paths.append(local_vp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737af58-3c86-4d3a-be4d-cf56c8811f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 PPI Uploader\n",
    "\n",
    "# Libraries\n",
    "from minio import Minio\n",
    "\n",
    "minioClient = Minio(\n",
    "    endpoint=conf_minio_endpoint,\n",
    "    access_key=secret_minio_access_key,\n",
    "    secret_key=secret_minio_secret_key,\n",
    "    secure=True,\n",
    ")\n",
    "\n",
    "for path in local_ppi_paths:\n",
    "    print(path)\n",
    "    # strip the leading \"/tmp/data\"\n",
    "    obj_key = pathlib.Path(*pathlib.Path(path).parts[3:])\n",
    "    obj_name = f\"{conf_minio_tutorial_prefix}/{conf_user_directory + param_user_number}/{obj_key}\"\n",
    "    print(obj_name)\n",
    "    minioClient.fput_object(\n",
    "        bucket_name=conf_minio_user_bucket_name,\n",
    "        object_name=obj_name,\n",
    "        file_path=path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dde149-7f94-4242-bf9f-40545ed2636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 VPTS Uploader\n",
    "# Libraries\n",
    "from minio import Minio\n",
    "\n",
    "minioClient = Minio(\n",
    "    endpoint=conf_minio_endpoint,\n",
    "    access_key=secret_minio_access_key,\n",
    "    secret_key=secret_minio_secret_key,\n",
    "    secure=True,\n",
    ")\n",
    "\n",
    "for path in local_vpts_paths:\n",
    "    # strip the leading \"/tmp/data\"\n",
    "    print(path)\n",
    "    obj_key = pathlib.Path(*pathlib.Path(path).parts[3:])\n",
    "    obj_name = f\"{conf_minio_tutorial_prefix}/{obj_key}\"\n",
    "    print(obj_name)\n",
    "    minioClient.fput_object(\n",
    "        bucket_name=conf_minio_user_bucket_name,\n",
    "        object_name=obj_name,\n",
    "        file_path=path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04576a73-fdc7-4fc5-9775-e5433a0d2e6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✔] Virtual environment 'my_venv' created at /home/koen-greuell/local_notebooks/my_venv\n",
      "Requirement already satisfied: pip in ./my_venv/lib/python3.13/site-packages (25.2)\n",
      "[✔] Pip upgraded\n",
      "Collecting ipykernel\n",
      "  Using cached ipykernel-6.30.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.40.15-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting ipylab\n",
      "  Using cached ipylab-1.1.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.2-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting xarray\n",
      "  Using cached xarray-2025.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-21.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting nbformat\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting jupyterlab_widgets\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting reprolab\n",
      "  Using cached reprolab-0.1.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting comm>=0.1.1 (from ipykernel)\n",
      "  Using cached comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting debugpy>=1.6.5 (from ipykernel)\n",
      "  Using cached debugpy-1.8.16-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting ipython>=7.23.1 (from ipykernel)\n",
      "  Using cached ipython-9.4.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting jupyter-client>=8.0.0 (from ipykernel)\n",
      "  Using cached jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
      "  Using cached jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
      "  Using cached matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting nest-asyncio>=1.4 (from ipykernel)\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting packaging>=22 (from ipykernel)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting psutil>=5.7 (from ipykernel)\n",
      "  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting pyzmq>=25 (from ipykernel)\n",
      "  Using cached pyzmq-27.0.2-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting tornado>=6.2 (from ipykernel)\n",
      "  Using cached tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting traitlets>=5.4.0 (from ipykernel)\n",
      "  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting botocore<1.41.0,>=1.40.15 (from boto3)\n",
      "  Using cached botocore-1.40.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
      "  Using cached s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.41.0,>=1.40.15->boto3)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore<1.41.0,>=1.40.15->boto3)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.15->boto3)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat)\n",
      "  Using cached fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jsonschema>=2.6 (from nbformat)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting jupyter-server>=2.0.0 (from reprolab)\n",
      "  Using cached jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab<5,>=4.0.0 (from reprolab)\n",
      "  Using cached jupyterlab-4.4.6-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting notebook>=7.0.0 (from reprolab)\n",
      "  Using cached notebook-7.4.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting httpx<1,>=0.25.0 (from jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jinja2>=3.0.3 (from jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached jupyter_lsp-2.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim>=0.2 (from jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting setuptools>=41.1.0 (from jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting anyio (from httpx<1,>=0.25.0->jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.25.0->jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nbconvert>=6.4.4 (from jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.25.0->jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting decorator (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting ipython-pygments-lexers (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached prompt_toolkit-3.0.51-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pygments>=2.4.0 (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting stack_data (from ipython>=7.23.1->ipykernel)\n",
      "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
      "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
      "  Using cached parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=3.0.3->jupyterlab<5,>=4.0.0->reprolab)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=2.6->nbformat)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat)\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat)\n",
      "  Using cached rpds_py-0.27.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel)\n",
      "  Using cached platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached mistune-3.1.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached cffi-1.17.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=2.0.0->reprolab)\n",
      "  Using cached types_python_dateutil-2.9.0.20250822-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting executing>=1.2.0 (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pure-eval (from stack_data->ipython>=7.23.1->ipykernel)\n",
      "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Using cached ipykernel-6.30.1-py3-none-any.whl (117 kB)\n",
      "Using cached boto3-1.40.15-py3-none-any.whl (140 kB)\n",
      "Using cached botocore-1.40.15-py3-none-any.whl (14.0 MB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached ipylab-1.1.0-py3-none-any.whl (101 kB)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Using cached pandas-2.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Using cached numpy-2.3.2-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached xarray-2025.8.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached pyarrow-21.0.0-cp313-cp313-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\n",
      "Using cached reprolab-0.1.0-py3-none-any.whl (66 kB)\n",
      "Using cached jupyterlab-4.4.6-py3-none-any.whl (12.3 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
      "Using cached jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Using cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
      "Using cached debugpy-1.8.16-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached ipython-9.4.0-py3-none-any.whl (611 kB)\n",
      "Using cached prompt_toolkit-3.0.51-py3-none-any.whl (387 kB)\n",
      "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Using cached parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached json5-0.12.1-py3-none-any.whl (36 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
      "Using cached jupyter_core-5.8.1-py3-none-any.whl (28 kB)\n",
      "Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jupyter_lsp-2.2.6-py3-none-any.whl (69 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
      "Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Using cached mistune-3.1.3-py3-none-any.whl (53 kB)\n",
      "Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached notebook-7.4.5-py3-none-any.whl (14.3 MB)\n",
      "Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Using cached pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Using cached platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
      "Using cached prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
      "Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "Using cached python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached pyzmq-27.0.2-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (840 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Using cached rpds_py-0.27.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Using cached tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (87 kB)\n",
      "Using cached cffi-1.17.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached types_python_dateutil-2.9.0.20250822-py3-none-any.whl (17 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
      "Using cached executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Using cached pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: webencodings, wcwidth, pytz, pure-eval, ptyprocess, fastjsonschema, widgetsnbextension, websocket-client, webcolors, urllib3, uri-template, tzdata, typing-extensions, types-python-dateutil, traitlets, tornado, tinycss2, soupsieve, sniffio, six, setuptools, send2trash, rpds-py, rfc3986-validator, pyzmq, pyyaml, python-json-logger, pygments, pycparser, pyarrow, psutil, prompt_toolkit, prometheus-client, platformdirs, pexpect, parso, pandocfilters, packaging, numpy, nest-asyncio, mistune, MarkupSafe, lark, jupyterlab_widgets, jupyterlab-pygments, jsonpointer, json5, jmespath, idna, h11, fqdn, executing, defusedxml, decorator, debugpy, comm, charset_normalizer, certifi, bleach, babel, attrs, async-lru, asttokens, terminado, stack_data, rfc3987-syntax, rfc3339-validator, requests, referencing, python-dateutil, matplotlib-inline, jupyter-core, jinja2, jedi, ipython-pygments-lexers, httpcore, cffi, beautifulsoup4, anyio, pandas, jupyter-server-terminals, jupyter-client, jsonschema-specifications, ipython, httpx, botocore, arrow, argon2-cffi-bindings, xarray, s3transfer, jsonschema, isoduration, ipywidgets, ipykernel, argon2-cffi, nbformat, ipylab, boto3, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, reprolab\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108/108\u001b[0m [reprolab]reprolab]jupyterlab]server]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 anyio-4.10.0 argon2-cffi-25.1.0 argon2-cffi-bindings-25.1.0 arrow-1.3.0 asttokens-3.0.0 async-lru-2.0.5 attrs-25.3.0 babel-2.17.0 beautifulsoup4-4.13.4 bleach-6.2.0 boto3-1.40.15 botocore-1.40.15 certifi-2025.8.3 cffi-1.17.1 charset_normalizer-3.4.3 comm-0.2.3 debugpy-1.8.16 decorator-5.2.1 defusedxml-0.7.1 executing-2.2.0 fastjsonschema-2.21.2 fqdn-1.5.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 ipykernel-6.30.1 ipylab-1.1.0 ipython-9.4.0 ipython-pygments-lexers-1.1.1 ipywidgets-8.1.7 isoduration-20.11.0 jedi-0.19.2 jinja2-3.1.6 jmespath-1.0.1 json5-0.12.1 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 jupyter-client-8.6.3 jupyter-core-5.8.1 jupyter-events-0.12.0 jupyter-lsp-2.2.6 jupyter-server-2.17.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.6 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab_widgets-3.0.15 lark-1.2.2 matplotlib-inline-0.1.7 mistune-3.1.3 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 nest-asyncio-1.6.0 notebook-7.4.5 notebook-shim-0.2.4 numpy-2.3.2 packaging-25.0 pandas-2.3.2 pandocfilters-1.5.1 parso-0.8.4 pexpect-4.9.0 platformdirs-4.3.8 prometheus-client-0.22.1 prompt_toolkit-3.0.51 psutil-7.0.0 ptyprocess-0.7.0 pure-eval-0.2.3 pyarrow-21.0.0 pycparser-2.22 pygments-2.19.2 python-dateutil-2.9.0.post0 python-json-logger-3.3.0 pytz-2025.2 pyyaml-6.0.2 pyzmq-27.0.2 referencing-0.36.2 reprolab-0.1.0 requests-2.32.5 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 rpds-py-0.27.0 s3transfer-0.13.1 send2trash-1.8.3 setuptools-80.9.0 six-1.17.0 sniffio-1.3.1 soupsieve-2.7 stack_data-0.6.3 terminado-0.18.1 tinycss2-1.4.0 tornado-6.5.2 traitlets-5.14.3 types-python-dateutil-2.9.0.20250822 typing-extensions-4.14.1 tzdata-2025.2 uri-template-1.3.0 urllib3-2.5.0 wcwidth-0.2.13 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0 widgetsnbextension-4.0.14 xarray-2025.8.0\n",
      "[✔] Installed packages: ipykernel, boto3, ipylab, pandas, numpy, xarray, requests, pyarrow, nbformat, pyyaml, ipywidgets, jupyterlab_widgets, reprolab\n",
      "Installed kernelspec my_venv_kernel in /home/koen-greuell/.local/share/jupyter/kernels/my_venv_kernel\n",
      "[✔] Kernel 'my_venv_kernel' registered for Jupyter\n",
      "\n",
      "🎉 Setup complete!\n",
      "➡ To use the virtual environment in Jupyter:\n",
      "   1. Restart your Jupyter server\n",
      "   2. Select kernel: ReproLab (my_venv)\n",
      "➡ ReproLab is automatically installed and ready to use!\n"
     ]
    }
   ],
   "source": [
    "from reprolab.environment import create_new_venv\n",
    "create_new_venv('my_venv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcdb6e61-f3f6-4713-bb61-0bfb11530948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying pip at: /home/koen-greuell/local_notebooks/my_venv/bin/pip\n",
      "Running command: /home/koen-greuell/local_notebooks/my_venv/bin/pip freeze\n",
      "Pip dependencies saved to requirements.txt\n",
      "Found 108 packages\n",
      "Not a Conda environment or not activated. Skipping Conda export.\n",
      "\n",
      "To recreate the environment:\n",
      "- For pip: Activate the virtual environment and run: `pip install -r requirements.txt`\n"
     ]
    }
   ],
   "source": [
    "from reprolab.environment import freeze_venv_dependencies\n",
    "freeze_venv_dependencies('my_venv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992046b5-8391-432e-a510-622d12eb3e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 17:19:52 - INFO - Starting experiment process\n",
      "2025-08-22 17:19:52 - INFO - Step 1: Saving all notebooks\n",
      "2025-08-22 17:19:52 - INFO - Attempting to save all Jupyter notebooks...\n",
      "2025-08-22 17:19:53 - INFO - ipylab save command executed successfully\n",
      "2025-08-22 17:19:53 - INFO - nbformat processing completed for 2 notebooks\n",
      "2025-08-22 17:19:53 - INFO - Jupyter save commands executed successfully\n",
      "2025-08-22 17:19:53 - INFO - All save methods completed\n",
      "2025-08-22 17:19:53 - INFO - Step 2: Determining next tag name\n",
      "2025-08-22 17:19:53 - INFO - Determining next tag name\n",
      "2025-08-22 17:19:53 - INFO - Fetching all tags from remote repositories\n",
      "2025-08-22 17:19:54 - INFO - Found 3 tags: ['v1.0.0', 'v1.1.0', 'v1.2.0']\n",
      "2025-08-22 17:19:54 - INFO - Latest tag: v1.2.0, next tag: v1.3.0\n",
      "2025-08-22 17:19:54 - INFO - Step 3: Committing with message: 'Project state before running experiment v1.3.0'\n",
      "2025-08-22 17:19:54 - INFO - Starting commit process with message: 'Project state before running experiment v1.3.0'\n",
      "2025-08-22 17:19:54 - INFO - Adding all files to staging area\n",
      "2025-08-22 17:19:54 - INFO - Starting process to add all files to git staging area\n",
      "2025-08-22 17:19:54 - INFO - Checking git status for all files\n",
      "2025-08-22 17:19:54 - INFO - Found 0 untracked files\n",
      "2025-08-22 17:19:54 - INFO - Found 0 modified files\n",
      "2025-08-22 17:19:54 - INFO - Found 0 deleted files\n",
      "2025-08-22 17:19:54 - INFO - No changes detected, but proceeding with add anyway\n",
      "2025-08-22 17:19:54 - INFO - Adding all files to git staging\n",
      "2025-08-22 17:19:54 - INFO - Successfully added all files to staging\n",
      "2025-08-22 17:19:54 - INFO - Checking for staged changes\n",
      "2025-08-22 17:19:54 - INFO - Staged 1 files for commit\n",
      "2025-08-22 17:19:54 - INFO - Creating commit with message: 'Project state before running experiment v1.3.0'\n",
      "2025-08-22 17:19:54 - INFO - Successfully committed: Project state before running experiment v1.3.0\n",
      "2025-08-22 17:19:54 - INFO - Successfully started experiment: v1.3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'v1.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reprolab.experiment import start_experiment\n",
    "start_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d84ab1-ae56-4ba2-9e5a-398f62b87e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 17:19:58 - INFO - Ending experiment process\n",
      "2025-08-22 17:19:58 - INFO - Step 1: Saving all notebooks\n",
      "2025-08-22 17:19:58 - INFO - Attempting to save all Jupyter notebooks...\n",
      "2025-08-22 17:19:58 - INFO - ipylab save command executed successfully\n",
      "2025-08-22 17:19:58 - INFO - nbformat processing completed for 2 notebooks\n"
     ]
    }
   ],
   "source": [
    "from reprolab.experiment import end_experiment\n",
    "end_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d8a8ca-3d68-47bf-b1dc-d6a60f885b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v1.2.0', 'v1.1.0', 'v1.0.0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reprolab.experiment import list_and_sort_git_tags\n",
    "list_and_sort_git_tags()\n",
    "# Pick your git tag, to download the reproducability package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07741359-b8ae-4e4b-acdd-6453274e3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprolab.experiment import download_reproducability_package\n",
    "download_reproducability_package('<git_tag>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReproLab (my_venv)",
   "language": "python",
   "name": "my_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.13.1"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "key": "language_info",
     "op": "remove"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
